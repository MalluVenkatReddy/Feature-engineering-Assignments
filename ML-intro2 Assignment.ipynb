{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae716d79-ffe4-4715-9e2f-c66956871d3f",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f07bb-90de-4d7f-ab28-b783cd6ac607",
   "metadata": {},
   "source": [
    "## Overfitting is an undesirable machine learning behavior that occurs when the machine learning model gives accurate predictions for training data but not for new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f03adc-5d55-49f9-8a65-53d3046466d9",
   "metadata": {},
   "source": [
    "## Your model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06376aff-edd6-4eca-972b-12dd80e2f44f",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac848ba7-d669-4c7a-9a7b-13ab04a799bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 10 techniques to avoid overfitting, Here we will discuss possible options to prevent overfitting, which helps improve the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f567954a-3aef-43be-8482-bc91e0f453bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (3637913401.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    1.Train with more data :\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "1.Train with more data :\n",
    "    With the increase in the training data, the crucial features to be extracted become prominent. \n",
    "    The model can recognize the relationship between the input attributes and the output variable. \n",
    "    The only assumption in this method is that the data to be fed into the model should be clean; otherwise, it would worsen the problem of overfitting.\n",
    "2. Data augmentation:\n",
    "    An alternative method to training with more data is data augmentation, which is less expensive and safer than the previous method. \n",
    "    Data augmentation makes a sample data look slightly different every time the model processes it. \n",
    "3. Addition of noise to the input data:\n",
    "    Another similar option as data augmentation is adding noise to the input and output data. \n",
    "    Adding noise to the input makes the model stable without affecting data quality and privacy while adding noise to the output makes the data more diverse. \n",
    "    Noise addition should be done in limit so that it does not make the data incorrect or too different.\n",
    "4. Feature selection:\n",
    "    Every model has several parameters or features depending upon the number of layers, number of neurons, etc.  \n",
    "    The model can detect many redundant features or features determinable from other features leading to unnecessary complexity. \n",
    "    We very well know that the more complex the model, the higher the chances of the model to overfit. \n",
    "5. Cross-validation:\n",
    "    Cross-validation is a robust measure to prevent overfitting. The complete dataset is split into parts. \n",
    "    In standard K-fold cross-validation, we need to partition the data into k folds. \n",
    "    Then, we iteratively train the algorithm on k-1 folds while using the remaining holdout fold as the test set. \n",
    "    This method allows us to tune the hyperparameters of the neural network or machine learning model and test it using completely unseen data.    \n",
    "6. Simplify data:\n",
    "    Till now, we have come across model complexity to be one of the top reasons for overfitting. \n",
    "    The data simplification method is used to reduce overfitting by decreasing the complexity of the model to make it simple enough that it does not overfit. \n",
    "    Some of the procedures include pruning a decision tree, reducing the number of parameters in a neural network, and using dropout on a neutral network. \n",
    "7. Regularization:\n",
    "    If overfitting occurs when a model is too complex, reducing the number of features makes sense. \n",
    "    Regularization methods like Lasso, L1 can be beneficial if we do not know which features to remove from our model. \n",
    "    Regularization applies a \"penalty\" to the input parameters with the larger coefficients, which subsequently limits the model's variance. \n",
    "8. Ensembling:\n",
    "    It is a machine learning technique that combines several base models to produce one optimal predictive model. \n",
    "    In Ensemble learning,  the predictions are aggregated to identify the most popular result. \n",
    "    Well-known ensemble methods include bagging and boosting, which prevents overfitting as an ensemble model is made from the aggregation of multiple models.\n",
    "9. Early stopping:\n",
    "    This method aims to pause the model's training before memorizing noise and random fluctuations from the data. \n",
    "    There can be a risk that the model stops training too soon, leading to underfitting. \n",
    "    One has to come to an optimum time/iterations the model should train. \n",
    "10. Adding dropout layers:\n",
    "    Large weights in a neural network signify a more complex network. Probabilistically dropping out nodes in the network is a simple and effective method to prevent overfitting. \n",
    "    In regularization, some number of layer outputs are randomly ignored or “dropped out” to reduce the complexity of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6629dcad-f3de-4d20-8442-f11a46597cf2",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ad594-f5ad-4728-a4a2-2c5bf9614b99",
   "metadata": {},
   "source": [
    "## Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. As a result, it may fail to find the best fit of the dominant trend in the data.\n",
    "\n",
    "## In the case of underfitting, the model is not able to learn enough from the training data, and hence it reduces the accuracy and produces unreliable predictions.\n",
    "\n",
    "## An underfitted model has high bias and low variance.\n",
    "## How to avoid underfitting:\n",
    "###  By increasing the training time of the model and By increasing the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a93233d-73fd-4f04-a019-206547c75d6f",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33be1a1-c172-4917-8701-f60d2b854aea",
   "metadata": {},
   "source": [
    "## For an accurate prediction of the model, algorithms need a low variance and low bias. But this is not possible because bias and variance are related to each other: If we decrease the variance, it will increase the bias. If we decrease the bias, it will increase the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae37eb7-5a62-4a1a-890e-bbbf01bf3bfa",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea528c74-0bf8-40ff-8868-67b5ac02d523",
   "metadata": {},
   "source": [
    "## We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b2221-fd59-452e-8cbc-f3af22b3c263",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d26940a-d8d1-4d98-924b-8cede5d981ef",
   "metadata": {},
   "source": [
    "## A linear machine-learning algorithm will exhibit high bias but low variance. On the other hand, a non-linear algorithm will exhibit low bias but high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c0117-dd21-4650-ba37-e91b6fdeb604",
   "metadata": {},
   "source": [
    "## Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance. When a data engineer modifies the ML algorithm to better fit a given data set, it will lead to low bias—but it will increase variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf996a-584a-4fed-b9f3-ff7cd8941339",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7d72d-417f-4e4f-962a-dc59c66aaa29",
   "metadata": {},
   "source": [
    "## Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28c17e-1b9c-49b1-8468-771678e90922",
   "metadata": {},
   "source": [
    "## Regularization methods like Lasso, L1 can be beneficial if we do not know which features to remove from our model. \n",
    "## Regularization applies a \"penalty\" to the input parameters with the larger coefficients, which subsequently limits the model's variance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
